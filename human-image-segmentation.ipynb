{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5506221,"sourceType":"datasetVersion","datasetId":1108926},{"sourceId":5506926,"sourceType":"datasetVersion","datasetId":1152755},{"sourceId":5770919,"sourceType":"datasetVersion","datasetId":1152595}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport pandas as pd\nimport glob\nfrom PIL import Image\nimport random\n!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths1 = glob.glob('/kaggle/input/segmentation-full-body-tiktok-dancing-dataset/segmentation_full_body_tik_tok_2615_img/segmentation_full_body_tik_tok_2615_img/images/*.png', recursive=True)\nimage_paths2 = glob.glob('/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/images/*.png', recursive=True)\nimage_paths3 = glob.glob('/kaggle/input/segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/images/*.png', recursive=True)\nmask_paths1 = glob.glob('/kaggle/input/segmentation-full-body-tiktok-dancing-dataset/segmentation_full_body_tik_tok_2615_img/segmentation_full_body_tik_tok_2615_img/masks/*.png', recursive=True)\nmask_paths2 = glob.glob('/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/masks/*.png', recursive=True)\nmask_paths3 = glob.glob('/kaggle/input/segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/masks/*.png', recursive=True)\n\nrandom.Random(0).shuffle(image_paths1)\nrandom.Random(0).shuffle(image_paths2)\nrandom.Random(0).shuffle(image_paths3)\nrandom.Random(0).shuffle(mask_paths1)\nrandom.Random(0).shuffle(mask_paths2)\nrandom.Random(0).shuffle(mask_paths3)\n\ntrain_mask_paths1 = mask_paths1[0: int(len(mask_paths1) * 0.9)]\ntrain_image_paths1 = image_paths1[0: int(len(image_paths1) * 0.9)]\nval_mask_paths1 = mask_paths1[int(len(mask_paths1) * 0.9): ]\nval_image_paths1 = image_paths1[int(len(image_paths1) * 0.9): ]\n\ntrain_mask_paths2 = mask_paths2[0: int(len(mask_paths2) * 0.9)]\ntrain_image_paths2 = image_paths2[0: int(len(image_paths2) * 0.9)]\nval_mask_paths2 = mask_paths2[int(len(mask_paths2) * 0.9): ]\nval_image_paths2 = image_paths2[int(len(image_paths2) * 0.9): ]\n\ntrain_mask_paths3 = mask_paths3[0: int(len(mask_paths3) * 0.9)]\ntrain_image_paths3 = image_paths3[0: int(len(image_paths3) * 0.9)]\nval_mask_paths3 = mask_paths3[int(len(mask_paths3) * 0.9): ]\nval_image_paths3 = image_paths3[int(len(image_paths3) * 0.9): ]\n\ntrain_image_paths = train_image_paths1 + train_image_paths2 + train_image_paths3\ntrain_mask_paths = train_mask_paths1 + train_mask_paths2 + train_mask_paths3\nval_image_paths = val_image_paths1 + val_image_paths2 + val_image_paths3\nval_mask_paths = val_mask_paths1 + val_mask_paths2 + val_mask_paths3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = smp.Unet(encoder_name=\"timm-mobilenetv3_large_100\", in_channels=3, classes=1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.unet(x)\n        x = self.sigmoid(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(data.Dataset):\n    def __init__(self, image_paths, mask_paths):\n        super().__init__()\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n\n    def transform(self, image, mask):\n        image = transforms.ToTensor()(image)\n        mask = transforms.ToTensor()(mask)\n        i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(256, 256))\n        image = TF.crop(image, i, j, h, w)\n        mask = TF.crop(mask, i, j, h, w)\n        return (image, mask)\n        \n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index]).convert('RGB')\n        mask = Image.open(self.mask_paths[index]).convert('L')\n        image, mask = self.transform(image, mask)\n        \n        return (image, mask)\n\n    def __len__(self):\n        return len(self.mask_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dice(nn.Module):\n    def __init__(self, eps=0):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, pred, y_true):\n        intersection = 2.0 * torch.sum(pred * y_true) + self.eps\n        union = torch.sum(y_true + pred) + self.eps\n        return intersection / union","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = nn.BCELoss()\ndice_loss = Dice(eps=1e-8)\nbatch_size = 30\nlearning_rate = 0.001\nnum_epochs = 3\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = SegmentationNet()\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\ntrain_dataset = MyDataset(train_image_paths, train_mask_paths)\nval_dataset = MyDataset(val_image_paths, val_mask_paths)\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, num_epochs + 1):\n    print('epoch:', epoch)\n    model.train()\n    for images, masks in train_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n        preds = model(images)\n        loss = 1 - dice_loss(preds, masks) + loss_func(preds, masks)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    model.eval() \n    with torch.no_grad():\n        for mode, loader in zip(['train', 'val'], [train_loader, val_loader]):\n            epoch_dice, epoch_loss, num_samples = 0, 0, 0\n            for images, masks in loader:\n                images = images.to(device)\n                masks = masks.to(device)\n                preds = model(images)\n                loss = loss_func(preds, masks)\n                preds[preds >= 0.5] = 1.0\n                preds[preds < 0.5] = 0.0\n                dice = dice_loss(preds, masks)\n                epoch_loss += loss.item() * masks.shape[0] \n                epoch_dice += dice.item() * masks.shape[0] \n                num_samples += masks.shape[0]\n\n            epoch_loss = epoch_loss / num_samples\n            epoch_dice = epoch_dice / num_samples\n            print(mode, '- loss:', f'{epoch_loss:.2}')\n            print(mode, '- dice:', f'{epoch_dice:.2}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    for image_path, mask_path in zip(val_image_paths[0: 20], val_mask_paths[0: 20]):\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n        image = transforms.ToTensor()(image).unsqueeze(0)\n        mask = transforms.ToTensor()(mask).unsqueeze(0)\n        _, _, h, w = image.shape\n        n_h = h // 256\n        n_w = w // 256\n        pred = torch.zeros((1, 1, h, w))\n        for i in range(n_h):\n            for j in range(n_w):\n                start_h = i * 256\n                end_h = start_h + 256\n                start_w = j * 256\n                end_w = start_w + 256\n                cropped_image = image[:, :, start_h: end_h, start_w: end_w]\n                cropped_image = cropped_image.to(device)\n                cropped_pred = model(cropped_image)\n                pred[:, :, start_h: end_h, start_w: end_w] = cropped_pred\n\n        pred[pred >= 0.5] = 1.0\n        pred[pred < 0.5] = 0.0\n        fig, axs = plt.subplots(1, 3)\n        axs[0].imshow(image.permute(0, 2, 3, 1)[0].detach().cpu().numpy())\n        axs[1].imshow(mask.permute(0, 2, 3, 1)[0].detach().cpu().numpy(), cmap='gray')\n        axs[2].imshow(pred.permute(0, 2, 3, 1)[0].detach().cpu().numpy(), cmap='gray')\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}