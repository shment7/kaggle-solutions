{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n!pip install nlpaug\nimport nlpaug.augmenter.word as naw","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T20:21:17.917032Z","iopub.execute_input":"2023-12-03T20:21:17.917415Z","iopub.status.idle":"2023-12-03T20:21:29.467287Z","shell.execute_reply.started":"2023-12-03T20:21:17.917384Z","shell.execute_reply":"2023-12-03T20:21:29.466276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf['sentiment'] = df['sentiment'].replace({'positive': 0, 'negative': 1})\ndf = df.sample(frac=1)\nn = df.shape[0]\ndf_train = df.iloc[0: int(n * 0.9)]\ndf_test = df.iloc[int(n * 0.9): ]\nx_test = df_test['review'].to_list()\ny_test = df_test['sentiment']\ndf_aug = df_train.copy()\naug = naw.RandomWordAug(action=\"swap\", aug_max=30)\ndf_aug['review'] = aug.augment(df_aug['review'].to_list())\naug = naw.SpellingAug(aug_max=30)\ndf_aug['review'] = aug.augment(df_aug['review'].to_list())\ndf_train = pd.concat([df_train, df_aug])\nx_train = df_train['review'].to_list()\ny_train = df_train['sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-12-03T20:21:29.469397Z","iopub.execute_input":"2023-12-03T20:21:29.469718Z","iopub.status.idle":"2023-12-03T20:26:11.710078Z","shell.execute_reply.started":"2023-12-03T20:21:29.469689Z","shell.execute_reply":"2023-12-03T20:26:11.709294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.transformer = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=2)\n\n    def forward(self, text):\n        x = self.transformer(**text).logits\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-03T20:26:11.711189Z","iopub.execute_input":"2023-12-03T20:26:11.711566Z","iopub.status.idle":"2023-12-03T20:26:11.717618Z","shell.execute_reply.started":"2023-12-03T20:26:11.711532Z","shell.execute_reply":"2023-12-03T20:26:11.716691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(data.Dataset):\n    def __init__(self, text, y=None):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n        self.x = self.tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n        self.y = torch.from_numpy(y.to_numpy())\n            \n    def __getitem__(self, index):\n        text = {k: v[index] for k, v in self.x.items()}\n        return (text, self.y[index])\n\n    def __len__(self):\n        return self.x['input_ids'].shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T20:26:11.719409Z","iopub.execute_input":"2023-12-03T20:26:11.719685Z","iopub.status.idle":"2023-12-03T20:26:11.729297Z","shell.execute_reply.started":"2023-12-03T20:26:11.719660Z","shell.execute_reply":"2023-12-03T20:26:11.728465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss()\nbatch_size = 8\nlearning_rate = 1e-5\nnum_epochs = 1\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = Model()\nprint(model)       \nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\ntrain_dataset = MyDataset(x_train, y_train)\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\ntest_dataset = MyDataset(x_test, y_test)\ntest_loader = data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T20:26:11.730455Z","iopub.execute_input":"2023-12-03T20:26:11.730716Z","iopub.status.idle":"2023-12-03T20:27:58.242207Z","shell.execute_reply.started":"2023-12-03T20:26:11.730693Z","shell.execute_reply":"2023-12-03T20:27:58.241438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, num_epochs + 1):  \n    model.train()\n    print('epoch:', epoch)\n    train_epoch_loss, train_n = 0, 0\n    for text, targets in train_loader:\n        text = {k:v.long().to(device) for k,v in text.items()}\n        targets = targets.long().to(device)\n        preds = model(text)\n        loss = loss_func(preds, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() \n        \n    model.eval() \n    with torch.no_grad():\n        for mode, loader in zip(['train', 'test'], [train_loader, test_loader]):\n            epoch_loss, currect, num_samples = 0, 0, 0\n            for text, targets in loader:\n                text = {k: v.long().to(device) for k,v in text.items()}\n                targets = targets.long().to(device)\n                preds = model(text)\n                loss = loss_func(preds, targets)\n                epoch_loss += loss.item() * targets.shape[0] \n                currect += (torch.argmax(preds, axis=1) == targets).sum()\n                num_samples += targets.shape[0]\n\n            epoch_loss = epoch_loss / num_samples\n            accuracy = currect / num_samples\n            print(mode, '- loss:', f'{epoch_loss:.2}', 'accuracy:', f'{accuracy:.4}')    ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T20:27:58.243361Z","iopub.execute_input":"2023-12-03T20:27:58.243665Z"},"trusted":true},"execution_count":null,"outputs":[]}]}